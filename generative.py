# -*- coding: utf-8 -*-
"""5055_hw6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bo402mlccCT1nSRI9L0ko-J2Fw3tUbD_
"""

import torch
import numpy as np
import bz2
from torch import nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import torch.optim as optim

dfile = bz2.BZ2File('xyData.bz2')
data = torch.from_numpy(np.load(dfile)).to(torch.float32)
dfile.close()
batch_size = 100


class XYDataset(Dataset):
    def __init__(self, xydata, transformation=None):
        self.xydata = xydata
        self.transformation = transformation

    def __len__(self):
        return self.xydata.shape[0]

    def __getitem__(self, idx):
        ret = self.xydata[idx, :, :, :]
        if self.transformation:
            ret = self.transformation(ret)

        return ret


trainset = XYDataset(data[:-10000, :, :, :])
train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                           shuffle=True)
testset = XYDataset(data[10000:, :, :, :])
test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False)


class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(32 * 16 * 16, 10)
        self.fc2 = nn.Linear(10, 1 * 16 * 16)  # Assuming the task is to output a single channel image


    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.relu(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = x.view(-1, 1, 16, 16)  # Reshape to the desired output format
        return x

    def sample(self, batchSize):
        with torch.no_grad():  # No need to compute gradients
            samples = torch.randn(batchSize, 1, 16, 16)  # This is a dummy implementation
            samples = self.forward(samples)  # Assuming the forward method can generate samples
        return samples

def train(net):
    criterion = nn.MSELoss()  # Assuming the task is a regression problem
    optimizer = optim.Adam(net.parameters(), lr=0.005)
    for epoch in range(10):  # Number of epochs is arbitrarily chosen
        running_loss = 0.0
        for i, data in enumerate(train_loader, 0):
            inputs = data
            # Assuming the task is to reconstruct the input, so input is also the target
            targets = data.clone()  

            optimizer.zero_grad()

            outputs = net(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            if i % 100 == 99:
                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))
                running_loss = 0.0

if __name__ == "__main__":
    net = NeuralNetwork()
    print(net)
    train(net)
    torch.save(net, 'generative.pth')